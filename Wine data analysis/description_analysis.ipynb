{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# pygal maps for country distribution\n",
    "import pygal.maps.world \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tasety = pd.read_csv(\"/Users/rwang/PycharmProjects/Kaggle_projects/Wine data analysis/description_tasety.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    79002\n",
       "0.0    71928\n",
       "Name: tasety, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_tasety.tasety.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take bag of 1-gram with tfidf\n",
    "tfidf = TfidfVectorizer()\n",
    "features = tfidf.fit_transform(description_tasety['description'])\n",
    "X = pd.DataFrame(features.todense(),\n",
    "            columns=tfidf.get_feature_names())\n",
    "target = description_tasety['tasety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = LogisticRegression(C=1)\n",
    "final_model.fit(X, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top positive word:  ('2020', 8.197181055377708)\n",
      "top positive word:  ('beautiful', 7.112505414958121)\n",
      "top positive word:  ('2013', 6.861941636273038)\n",
      "top positive word:  ('years', 6.753032763158685)\n",
      "top positive word:  ('long', 6.345327606743431)\n",
      "------------------------------------------------------------\n",
      "top negative word:  ('simple', -8.262725460631593)\n",
      "top negative word:  ('thin', -6.098004094330263)\n",
      "top negative word:  ('lacks', -6.03083681948909)\n",
      "top negative word:  ('everyday', -5.885572294866951)\n",
      "top negative word:  ('86', -5.619655482522711)\n"
     ]
    }
   ],
   "source": [
    "# find the top positive words and negative words\n",
    "feature_to_coef = {word: coef for word, coef in zip(tfidf.get_feature_names(), final_model.coef_[0])}\n",
    "\n",
    "positives = sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)\n",
    "for best_positive in positives[:5]:\n",
    "    print (\"top positive word: \", best_positive)\n",
    "\n",
    "print('-'*60)\n",
    "negatives = sorted(feature_to_coef.items(), key=lambda x: x[1])\n",
    "    \n",
    "for best_negative in negatives[:5]:\n",
    "    print (\"top negative word: \", best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
